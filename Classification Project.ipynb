import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Scikit-learn imports for preprocessing, feature selection, and modeling
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import (
    AdaBoostClassifier,
    BaggingClassifier,
    RandomForestClassifier,
    ExtraTreesClassifier,
    VotingClassifier,
)
from sklearn.feature_selection import RFE, SelectKBest
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.metrics import f1_score
from sklearn.decomposition import PCA
from pickle import dump, load

# Suppress warnings related to max_iter for LogisticRegression
warnings.filterwarnings("ignore")

# --- 1. Data Loading and Preparation ---

# Note: This script assumes 'pima-indians-diabetes.csv' is in the working directory.
# This dataset is commonly available, e.g., on Kaggle or the UCI ML Repository.
FILENAME = 'pima-indians-diabetes.csv'
NAMES = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
SEED = 7 # Consistent random state for reproducibility
TEST_SIZE = 0.33
NUM_FOLDS = 10

def load_data(filename, names):
    """Loads the dataset and separates features (X) from target (Y)."""
    try:
        data = pd.read_csv(filename, names=names)
    except FileNotFoundError:
        print(f"Error: The file '{filename}' was not found. Please ensure it is in the current directory.")
        return None, None
    
    array = data.values
    X = array[:, 0:8]
    Y = array[:, 8]
    return X, Y

X, Y = load_data(FILENAME, NAMES)

if X is None:
    exit()

print("--- Data Loaded ---")
print(f"Dataset Shape: {X.shape}")

# --- 2. Exploratory Analysis Snippets (Descriptive Stats) ---
# Note: Visualizations (histograms, scatter matrix) are omitted in the final
# script for cleaner execution, but their logic is shown in the model comparison.
dataframe = pd.DataFrame(X, columns=NAMES[:-1])
print("\n--- Descriptive Statistics (First 5 rows) ---")
print(dataframe.head())
print("\n--- Class Proportions ---")
print(pd.Series(Y).value_counts())


# --- 3. Feature Engineering / Preprocessing Examples ---

print("\n--- Feature Importance (Extra Trees Classifier) ---")
model_etc = ExtraTreesClassifier(random_state=SEED)
model_etc.fit(X, Y)
print(model_etc.feature_importances_)

print("\n--- Feature Selection (RFE with Logistic Regression) ---")
model_lr_rfe = LogisticRegression(max_iter=500, random_state=SEED)
rfe = RFE(model_lr_rfe, n_features_to_select=4)
fit_rfe = rfe.fit(X, Y)
print(f"Selected Features (Boolean): {fit_rfe.support_}")
print(f"Feature Ranks: {fit_rfe.ranking_}")

# --- 4. Model Evaluation and Comparison ---

print("\n--- Algorithm Comparison (10-Fold Cross-Validation) ---")

# Define the set of models to evaluate
models = []
models.append(('LR', LogisticRegression(max_iter=500, random_state=SEED)))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier(random_state=SEED)))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC(random_state=SEED)))
models.append(('ADA', AdaBoostClassifier(n_estimators=100, random_state=SEED)))
models.append(('BAG', BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=SEED), n_estimators=100, random_state=SEED)))
models.append(('RFC', RandomForestClassifier(n_estimators=100, max_features=3, random_state=SEED)))
# Voting Ensemble using the best performing simple models
estimators_voting = [
    ('logistic', LogisticRegression(max_iter=500, random_state=SEED)),
    ('cart', DecisionTreeClassifier(random_state=SEED)),
    ('svm', SVC(random_state=SEED, probability=True))
]
models.append(('VOT', VotingClassifier(estimators_voting, voting='soft')))


results = []
names = []
scoring = 'accuracy'
for name, model in models:
    kfold = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)
    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(name)
    msg = f"{name}: {cv_results.mean()*100.0:.4f}% ({cv_results.std():.4f})"
    print(msg)

# Generate Boxplot for comparison fig = plt.figure(figsize=(10, 6))
fig.suptitle('Algorithm Comparison - Accuracy Scores')
ax = fig.add_subplot(111)
plt.boxplot(results, showfliers=False)
ax.set_xticklabels(names)
plt.ylabel('Cross-Validation Accuracy Score')
plt.grid(axis='y', linestyle='--')
plt.show()

# --- 5. Pipelines and Final Model Training ---

print("\n--- Pipeline Evaluation (Standardize + LR) ---")
estimators_pipeline = []
estimators_pipeline.append(('standardize', StandardScaler()))
estimators_pipeline.append(('logistic', LogisticRegression(max_iter=500, random_state=SEED)))
model_pipe = Pipeline(estimators_pipeline)
kfold = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)
results_pipe = cross_val_score(model_pipe, X, Y, cv=kfold)
print(f"Pipeline Accuracy: {results_pipe.mean()*100.0:.4f}%")


print("\n--- Training and Saving Final Model (Logistic Regression) ---")

# Best practice is to scale the data before fitting Logistic Regression
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=TEST_SIZE, random_state=SEED)

# Use the best pipeline (StandardScaler + LR) for the final model
final_model = Pipeline([
    ('standardize', StandardScaler()),
    ('logistic', LogisticRegression(max_iter=500, random_state=SEED))
])

final_model.fit(X_train, Y_train)

# Save the model to disk
MODEL_FILENAME = 'finalized_diabetes_model.sav'
try:
    dump(final_model, open(MODEL_FILENAME, 'wb'))
    print(f"Model successfully saved to '{MODEL_FILENAME}'")
except Exception as e:
    print(f"Error saving model: {e}")

# Load the model from disk and test
try:
    loaded_model = load(open(MODEL_FILENAME, 'rb'))
    result = loaded_model.score(X_test, Y_test)
    print(f"Loaded model accuracy on test set: {result*100.0:.4f}%")

    # Example Prediction
    example_input = np.array([[6, 148, 72, 35, 0, 33.6, 0.627, 50]]) # Example data point
    example_input_scaled = loaded_model.named_steps['standardize'].transform(example_input)
    prediction = loaded_model.named_steps['logistic'].predict(example_input_scaled)
    print(f"Prediction for example input: {'Positive (1)' if prediction[0] == 1 else 'Negative (0)'}")
    
except Exception as e:
    print(f"Error loading or testing saved model: {e}")

print("\nScript execution complete.")
